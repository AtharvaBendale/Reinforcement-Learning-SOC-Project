{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Monte Carlo ES\n",
    "\n",
    "- Here you are given an executable that represents the Markov Decision Process. The executable is named [```MDP```](./MDP).\n",
    "\n",
    "- You can query the number of states and actions of the MDP with ```./MDP states``` and ```./MDP actions```. The discount factor of the MDP can be obtained with ```./MDP gamma```.\n",
    "\n",
    "- To start interacting with the MDP, run ```./MDP <starting state>```. At every iteration, the executable will display the current state and current return of the MDP, and ask you to choose an action, after which it will give a reward, and transition to a new state.\n",
    "\n",
    "- You must implement the Monte Carlo ES algorithm that learns the optimal policy of the MDP by simulating episodes with exploring starts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Solving an MDP with Monte Carlo ES (Exploring Starts) algorithm\n",
    "This program uses subprocess library to operate the given enviroment (an .exe file) and obtain the optimal policy of this particular MDP\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloES:\n",
    "    def __init__(self) -> None:\n",
    "        process = subprocess.Popen(['./a.out', \"states\"], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        stdout, stderr = process.communicate()\n",
    "        stdout = stdout.decode('utf-8')\n",
    "        self.num_states = int(stdout)\n",
    "        process = subprocess.Popen(['./a.out', \"actions\"], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        stdout, stderr = process.communicate()\n",
    "        stdout = stdout.decode('utf-8')\n",
    "        self.num_actions = int(stdout)\n",
    "        process = subprocess.Popen(['./a.out', \"gamma\"], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        stdout, stderr = process.communicate()\n",
    "        stdout = stdout.decode('utf-8')\n",
    "        self.gamma = float(stdout)\n",
    "        self.Q = np.zeros((self.num_states, self.num_actions))\n",
    "        self.policy = np.array(self.num_states)\n",
    "        self.returns = [[[] for j in range(self.num_actions)] for i in range(self.num_states)]\n",
    "        self.actions = []\n",
    "        print(\"Number of states : \", self.num_states)\n",
    "        print(\"Number of action : \", self.num_actions)\n",
    "        print(\"Gamma : \", self.gamma)\n",
    "    def action(self, episode_length=1000):\n",
    "        self.actions = []\n",
    "        state = random.randint(0,self.num_states-1)\n",
    "        process = subprocess.Popen(['./a.out', str(state)], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        for _ in range(episode_length):\n",
    "            action = random.randint(0,4)\n",
    "            self.actions.append(action)\n",
    "            process.stdin.write((str(action)+'\\n').encode('utf-8'))\n",
    "        stdout, stderr = process.communicate()\n",
    "        stdout = stdout.decode('utf-8')\n",
    "        state_list = stdout.split(\"\\n\")[:-3:3][:-1]\n",
    "        state_list = [int(states.split(\":\")[-1][1:]) for states in state_list]\n",
    "        reward_list = stdout.split(\"\\n\")[2:-3:3]\n",
    "        reward_list = [float(rewards.split(\":\")[-1][1:]) for rewards in reward_list]\n",
    "        for j in range(1,len(reward_list)):\n",
    "            reward_list[-j-1] += self.gamma*reward_list[-j]\n",
    "        visited = [[0 for j in range(self.num_actions)] for i in range(self.num_states)]\n",
    "        for j in range(len(self.actions)):\n",
    "            if not any(0 in sublist for sublist in visited):\n",
    "                break\n",
    "            if not visited[state_list[j]][self.actions[j]]:\n",
    "                visited[state_list[j]][self.actions[j]] = 1\n",
    "                self.returns[state_list[j]][self.actions[j]].append(reward_list[j])\n",
    "                self.Q[state_list[j]][self.actions[j]] = sum(self.returns[state_list[j]][self.actions[j]])/len(self.returns[state_list[j]][self.actions[j]])\n",
    "        for j, state in enumerate(state_list):\n",
    "            pass\n",
    "    def train(self, iterations=1000, episode_length=100):\n",
    "        for _ in range(iterations):\n",
    "            self.action(episode_length)\n",
    "        self.policy  = np.argmax(np.array(self.Q), axis=1)\n",
    "    def showQ(self):\n",
    "        print(\"\\nq( s | a ) values for all states & action pairs\")\n",
    "        for i in range(self.num_states):\n",
    "            print(*self.Q[i], sep=\" \")\n",
    "        print()\n",
    "    def show_policy(self):\n",
    "        print(\"States : Optimal actions\")\n",
    "        for i in range(self.num_states):\n",
    "            print(i, \" : \", self.policy[i])\n",
    "    def evaluate(self, episode_length=100):\n",
    "        state = random.randint(0,self.num_states-1)\n",
    "        process = subprocess.Popen(['./a.out', str(state)], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        for i in range(episode_length):\n",
    "            state = random.randint(0,self.num_actions-1)\n",
    "            process.stdin.write((str(state)+\"\\n\").encode('utf-8'))\n",
    "        stdout, stderr = process.communicate()\n",
    "        stdout = stdout.decode('utf-8')\n",
    "        value = stdout.split(\"\\n\")[-3].split(\":\")[-1][1:]\n",
    "        print(f\"\\nFinal return by policy after an episode of length {episode_length}\")\n",
    "        print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states :  10\n",
      "Number of action :  5\n",
      "Gamma :  0.75\n",
      "\n",
      "q( s | a ) values for all states & action pairs\n",
      "-0.42273012472566246 -0.7045569931349105 -0.07574218242754943 -0.20957586518466448 -0.20473862357551462\n",
      "0.06018118893736839 -0.7022091712794746 0.2892273414944502 -0.6566501341662108 -0.1569936495018607\n",
      "0.9029657617242225 -0.4760514179437689 0.2139081024423429 -0.2900415846071898 -0.9331258582565156\n",
      "-0.38479299280745644 -0.3117918134732012 -0.7011903776190129 0.09414301096148749 0.14916020938681246\n",
      "0.11435157332304996 -0.5618629418520863 -0.3554294365062332 0.42339781900182716 -0.8788404961289737\n",
      "-0.3950244984071992 -0.13083771763624447 -0.50112543458138 -0.28550886122375907 -0.6078089630433554\n",
      "0.4331553759255612 -0.03138320480640642 0.17543666121777204 -0.4699798377929238 -0.6161913682290096\n",
      "-0.20896874571112237 -0.3689946976400283 0.01645003893273207 -1.2351503568241327 -0.2184958179508261\n",
      "-0.0546690550576868 -0.10468650189759063 0.5384559443989935 -0.6124941650788289 0.4022217566260155\n",
      "-0.3167747546359769 0.6151859686816087 -0.14437977537457564 0.14791659386433903 -0.9153046634393945\n",
      "\n",
      "States : Optimal actions\n",
      "0  :  2\n",
      "1  :  2\n",
      "2  :  0\n",
      "3  :  4\n",
      "4  :  3\n",
      "5  :  1\n",
      "6  :  0\n",
      "7  :  2\n",
      "8  :  2\n",
      "9  :  1\n",
      "\n",
      "Final return by policy after an episode of length 100\n",
      "2.22153\n"
     ]
    }
   ],
   "source": [
    "obj = MonteCarloES()\n",
    "obj.train()\n",
    "obj.showQ()\n",
    "obj.show_policy()\n",
    "obj.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
