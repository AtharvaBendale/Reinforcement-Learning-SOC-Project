{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IIB: Solving Tic-Tac-Toe using $\\varepsilon$-soft On-Policy Techniques\n",
    "\n",
    "- Here, you will implement an On-Policy algorithm using $\\varepsilon$-soft policies in order to make a Tic-Tac-Toe engine capable of playing Tic-Tac-Toe on an $N \\times N$ board.\n",
    "\n",
    "- You can read about the algorithm to be used in [my notes](../report.pdf) or in [Sutton and Barto](../SuttonBarto.pdf).\n",
    "\n",
    "- The Tic-Tac-Toe engine must simulate episodes before hand and used the knowledge it gained to play against the human player.\n",
    "\n",
    "- Since Tic-Tac-Toe is a two player game, the opponent must be simulated as part of the environment to convert this into an MDP. This can be done in two ways, either you can make the opponent another instance of the engine, or the opponent can play randomly. You will implement both these techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For playing TicTacToe run all the cells and wait till the model is trained\n",
    "You will be asked to enter number corresponding to the boards where you want to make your move, for example in 1 3x3 TicTacToe:\n",
    "1 | 2 | 3\n",
    "4 | 5 | 6\n",
    "7 | 8 | 9\n",
    "The model trains for a 3x3 TicTacToe by default, you can definitely modify the values(of N, number of iterations etc) for your convenience but training model for bigger N will take lots of time\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_2D_array_index(arr_3d, target_arr) -> int:\n",
    "    indices = np.where(np.all(arr_3d == target_arr, axis=(1, 2)))\n",
    "    if indices[0].size > 0:\n",
    "        return int(indices[0][0])\n",
    "    else: \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self, N=3):\n",
    "        self.N = N\n",
    "\n",
    "    def win_check(self, board) -> int:\n",
    "        list_row = np.sum(board, axis=0)\n",
    "        list_col= np.sum(board, axis=1)\n",
    "        list_diag = np.array([sum([board[i][i] for i in range(self.N)]), sum([board[i][self.N-1-i] for i in range(self.N)])])\n",
    "\n",
    "        if np.any(list_row==self.N) or np.any(list_col==self.N) or np.any(list_diag==self.N):\n",
    "            return 1\n",
    "        elif np.any(list_row==-self.N) or np.any(list_col==-self.N) or np.any(list_diag==-self.N):\n",
    "            return -1\n",
    "        elif np.all(board!=0):\n",
    "            return 0\n",
    "        else:\n",
    "            return None\n",
    "    def valid_moves(self, board):\n",
    "        if self.win_check(board) is not None:\n",
    "            return np.array([])\n",
    "        move_list = np.array([])\n",
    "        for i in range(self.N):\n",
    "            for j in range(self.N):\n",
    "                if board[i][j] == 0:\n",
    "                    move_list = np.append(move_list, np.array([self.N*i+j]), axis=0)\n",
    "        return move_list\n",
    "    \n",
    "    def make_move(self, board, move:float) :\n",
    "        move = int(move)\n",
    "        i, j = move//self.N, move%self.N\n",
    "        boardone = np.copy(board)\n",
    "        if np.sum(board)==0:\n",
    "            current_player=1\n",
    "        else:\n",
    "            current_player=-1\n",
    "        if boardone[i][j] != 0:\n",
    "            raise ValueError(\"Invalid move: Already occupied.\")\n",
    "        boardone[i][j] = current_player\n",
    "        return boardone\n",
    "        \n",
    "    def game_print_board(self, board):\n",
    "        for i in board:\n",
    "            str = \"|\"\n",
    "            for j in i:\n",
    "                if j == 1:\n",
    "                    str = str + \"X|\"\n",
    "                elif j == -1:\n",
    "                    str = str + \"O|\"\n",
    "                else:\n",
    "                    str = str + \" |\"\n",
    "            print(str)\n",
    "            print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Initializer(TicTacToe):\n",
    "    def __init__(self, N=3):\n",
    "        super().__init__(N)\n",
    "        self.states = np.zeros((1, self.N, self.N))\n",
    "        self.returns = [[[] for _ in range(self.N**2)]]\n",
    "        self.Q = np.zeros((1,self.N**2))\n",
    "        self.policy = np.random.dirichlet(np.ones(self.N**2)).reshape(1,-1)\n",
    "        self.recursive(np.zeros((self.N,self.N)), 1)\n",
    "    \n",
    "    def recursive(self, board, current_player):\n",
    "\n",
    "        valid_moves = self.valid_moves(board)\n",
    "        for move in valid_moves:\n",
    "            new_board = self.make_move(board, move)\n",
    "            if find_2D_array_index(self.states, new_board) is None:\n",
    "    \n",
    "                self.states = np.append(self.states, new_board.reshape(1, self.N, self.N), axis=0)\n",
    "                self.returns.append([[] for _ in range(self.N**2)])\n",
    "                win = self.win_check(new_board)\n",
    "                if win is None:\n",
    "                    self.Q = np.append(self.Q, np.zeros((1,self.N**2)), axis=0)\n",
    "                    \n",
    "                    new_valid_moves = self.valid_moves(new_board)\n",
    "                    random_array = np.random.rand(self.N**2)\n",
    "                    mask = np.isin(np.arange(self.N**2), new_valid_moves, invert=True)\n",
    "                    random_array[mask] = 0\n",
    "                    random_array = random_array/np.sum(random_array)\n",
    "                    \n",
    "                    self.policy = np.append(self.policy, random_array.reshape(1,-1), axis=0)\n",
    "                    self.recursive(new_board, -current_player)\n",
    "                else:\n",
    "                    self.Q = np.append(self.Q, win*np.ones((1,self.N**2)), axis=0)\n",
    "                    self.policy = np.append(self.policy, np.empty((1,self.N**2)), axis=0)\n",
    "            \n",
    "    def return_values(self):\n",
    "        return self.states, self.returns, self.Q, self.policy\n",
    "\n",
    "    def print_stats(self):\n",
    "        print(\"Total Number of states : \", len(self.states))\n",
    "        print(\"First state : \\n\", self.states[0])\n",
    "        print(\"Policy for first state : \\n\", self.policy[0])\n",
    "        print(\"Q for first state : \\n\", self.Q[0])\n",
    "        print(\"Return for first state so far : \\n\", self.returns[0])\n",
    "        \n",
    "        print(\"\\nSecond state : \\n\", self.states[1000])\n",
    "        print(\"Policy for second state : \\n\", self.policy[1000])\n",
    "        print(\"Q for second state : \\n\", self.Q[1000])\n",
    "        print(\"Return for second state so far : \\n\", self.returns[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLAgent(Initializer):\n",
    "    def __init__(self, N=3, gamma=0.75, epsilon=0.8):\n",
    "        super().__init__(N)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.episode_states = np.array([])\n",
    "        self.episode_actions = np.array([])\n",
    "\n",
    "    def train(self, iterations=200000):\n",
    "        sim_player = 1\n",
    "        counter = 0\n",
    "        for num_episode in range(iterations):\n",
    "            counter = counter + 1\n",
    "            self.simulate_episode(sim_player)\n",
    "            sim_player = -sim_player\n",
    "            if counter%(iterations//10) == 0:\n",
    "                print(f\"{counter}/{iterations} done !\")\n",
    "        \n",
    "\n",
    "    def simulate_episode(self, sim_player):\n",
    "        self.episode_states = np.array([])\n",
    "        self.episode_actions = np.array([])\n",
    "        self.opponent_actions = np.array([])\n",
    "        player = 1\n",
    "        board = np.zeros((self.N, self.N))\n",
    "        while self.win_check(board) is None:\n",
    "            valid_moves = self.valid_moves(board)\n",
    "            index = int(find_2D_array_index(self.states, board))\n",
    "            if player == sim_player:\n",
    "                mover = np.random.choice(np.arange(self.N**2), p=self.policy[index])\n",
    "                self.episode_states = np.append(self.episode_states, np.array([index]), axis=0)\n",
    "                self.episode_actions = np.append(self.episode_actions, np.array([mover]), axis=0)\n",
    "            else:\n",
    "                mover = np.random.choice(np.arange(self.N**2), p=self.policy[index])\n",
    "                self.opponent_actions = np.append(self.opponent_actions, np.array([mover]), axis=0)\n",
    "\n",
    "            board = self.make_move(board, mover)\n",
    "            player = -player\n",
    "        self.update_policy(sim_player)\n",
    "    \n",
    "    def update_policy(self, sim_player:int):\n",
    "        self.episode_states = self.episode_states[::-1]\n",
    "        self.episode_actions = self.episode_actions[::-1]\n",
    "        for k in range(len(self.episode_states)):\n",
    "            index = int(self.episode_states[k])\n",
    "            board = self.states[index]\n",
    "            move = int(self.episode_actions[k])\n",
    "            resulting_board = self.make_move(board, move)\n",
    "            next_valid_moves = self.valid_moves(resulting_board)\n",
    "            if self.opponent_actions[-1] in next_valid_moves:\n",
    "                win = self.win_check(self.make_move(resulting_board, self.opponent_actions[-1]))\n",
    "            else:\n",
    "                win = self.win_check(resulting_board)\n",
    "            if win==sim_player:\n",
    "                self.returns[index][move].append(1)\n",
    "            elif win==(-sim_player):\n",
    "                self.returns[index][move].append(-1)\n",
    "            elif win==0:\n",
    "                self.returns[index][move].append(0)\n",
    "            else:\n",
    "                resulting_index = int(self.episode_states[k-1])\n",
    "                previous_move = int(self.episode_actions[k-1])\n",
    "                self.returns[index][move].append(self.returns[resulting_index][previous_move][-1]*self.gamma)\n",
    "            self.Q[index][move] = sum(self.returns[index][move])/len(self.returns[index][move])\n",
    "            positive_indices = np.where(self.policy[index]> 0)[0]\n",
    "            max_index = positive_indices[np.argmax(self.Q[index][positive_indices])]\n",
    "            \n",
    "            self.policy[index] = np.sign(self.policy[index])\n",
    "            non_zero_indices = np.nonzero(self.policy[index])[0]\n",
    "            non_max_indices = non_zero_indices[non_zero_indices != max_index]\n",
    "            value_buf = np.sum(self.policy[index])\n",
    "            self.policy[index][max_index] = 1 - self.epsilon + self.epsilon / value_buf\n",
    "            self.policy[index][non_max_indices] = self.epsilon / value_buf\n",
    "\n",
    "    def optimal_policy(self, board):\n",
    "        index = find_2D_array_index(self.states, board)\n",
    "        val = np.sum(board)\n",
    "        if val==0:\n",
    "            current_player=1\n",
    "        elif val==1:\n",
    "            current_player=-1\n",
    "        max_index = np.argmax(self.policy[index])\n",
    "        return max_index\n",
    "    \n",
    "    def show_policy(self):\n",
    "        for i in range(100):\n",
    "            self.print_board(self.states[-i-1])\n",
    "            print(self.policy[-i-1], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeEngine:\n",
    "\n",
    "    def __init__(self, N=3, player=1):\n",
    "        self.N= N\n",
    "        self.player = player\n",
    "        self.game(self.N, self.player)\n",
    "    \n",
    "    def game(self, N=3, player=1):\n",
    "        Agent = RLAgent(N)\n",
    "        Agent.train()\n",
    "        a = \"Y\"\n",
    "        while True:\n",
    "            if a == \"Y\":\n",
    "                board = np.zeros((N,N))\n",
    "                Agent.game_print_board(board)\n",
    "                while Agent.win_check(board) is None:\n",
    "                    print(\"Your move -\")\n",
    "                    move = int(input(\"Enter cell number : \"))\n",
    "                    move = move - 1\n",
    "                    board = Agent.make_move(board, move)\n",
    "                    Agent.game_print_board(board)\n",
    "                    if Agent.win_check(board) is not None : break\n",
    "                    print(\"Computer's move - \")\n",
    "                    board = Agent.make_move(board, Agent.optimal_policy(board))\n",
    "                    Agent.game_print_board(board)\n",
    "                win = Agent.win_check(board)\n",
    "                if win == 1:\n",
    "                    print(\"Congrats!! You won :)\")\n",
    "                elif win == -1:\n",
    "                    print(\"Sorry you lost against the might of my model\")\n",
    "                else:\n",
    "                    print(\"OHHH... It's a DRAW!!\")\n",
    "            elif a == \"n\":\n",
    "                break\n",
    "            else:\n",
    "                print(\"Invalid Input!!\")\n",
    "            a = input(\"Do you want to play again? (Y/n)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/200000 done !\n",
      "40000/200000 done !\n",
      "60000/200000 done !\n",
      "80000/200000 done !\n",
      "100000/200000 done !\n",
      "120000/200000 done !\n",
      "140000/200000 done !\n",
      "160000/200000 done !\n",
      "180000/200000 done !\n",
      "200000/200000 done !\n",
      "| | | |\n",
      "------\n",
      "| | | |\n",
      "------\n",
      "| | | |\n",
      "------\n",
      "Your move -\n",
      "| | | |\n",
      "------\n",
      "| |X| |\n",
      "------\n",
      "| | | |\n",
      "------\n",
      "Computer's move - \n",
      "| | | |\n",
      "------\n",
      "| |X| |\n",
      "------\n",
      "| | |O|\n",
      "------\n",
      "Your move -\n",
      "|X| | |\n",
      "------\n",
      "| |X| |\n",
      "------\n",
      "| | |O|\n",
      "------\n",
      "Computer's move - \n",
      "|X| |O|\n",
      "------\n",
      "| |X| |\n",
      "------\n",
      "| | |O|\n",
      "------\n",
      "Your move -\n",
      "|X| |O|\n",
      "------\n",
      "| |X|X|\n",
      "------\n",
      "| | |O|\n",
      "------\n",
      "Computer's move - \n",
      "|X| |O|\n",
      "------\n",
      "|O|X|X|\n",
      "------\n",
      "| | |O|\n",
      "------\n",
      "Your move -\n",
      "|X|X|O|\n",
      "------\n",
      "|O|X|X|\n",
      "------\n",
      "| | |O|\n",
      "------\n",
      "Computer's move - \n",
      "|X|X|O|\n",
      "------\n",
      "|O|X|X|\n",
      "------\n",
      "| |O|O|\n",
      "------\n",
      "Your move -\n",
      "|X|X|O|\n",
      "------\n",
      "|O|X|X|\n",
      "------\n",
      "|X|O|O|\n",
      "------\n",
      "OHHH... It's a DRAW!!\n"
     ]
    }
   ],
   "source": [
    "Engine = TicTacToeEngine(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
